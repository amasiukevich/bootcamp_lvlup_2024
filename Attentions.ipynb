{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcapugGJ2K9l",
        "outputId": "9aec0fad-eea2-43b1-c99b-f0b8c4757591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 3, 5])\n",
            "Attention weights shape: torch.Size([2, 3, 4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))  # Dot product of query and key\n",
        "        scores = scores / torch.sqrt(torch.tensor(query.size(-1), dtype=torch.float32)) # Scale the scores\n",
        "\n",
        "        attention_weights = F.softmax(scores, dim=-1) # Apply softmax to get attention weights\n",
        "\n",
        "        output = torch.matmul(attention_weights, value) # Compute the weighted sum using attention weights\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "# Let's test our implementation\n",
        "batch_size = 2\n",
        "query_len = 3\n",
        "key_len = 4\n",
        "d_model = 5\n",
        "\n",
        "# Random input tensors\n",
        "query = torch.randn(batch_size, query_len, d_model)\n",
        "key = torch.randn(batch_size, key_len, d_model)\n",
        "value = torch.randn(batch_size, key_len, d_model)\n",
        "\n",
        "# Initialize and apply DotProductAttention\n",
        "attention = DotProductAttention()\n",
        "output, attention_weights = attention(query, key, value)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Attention weights shape:\", attention_weights.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads.\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.output_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections for each head\n",
        "        query = self.query_linear(query).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        key = self.key_linear(key).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        value = self.value_linear(value).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # Dot-product attention\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n",
        "\n",
        "        # Apply mask if provided\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply dropout if needed\n",
        "\n",
        "        # Compute the weighted sum using attention weights\n",
        "        output = torch.matmul(attention_weights, value)\n",
        "\n",
        "        # Concatenate heads and apply output linear transformation\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.output_linear(output)\n",
        "\n",
        "        return output, attention_weights\n"
      ],
      "metadata": {
        "id": "tpuyG2xP2OSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "query_len = 3\n",
        "key_len = 4\n",
        "d_model = 16\n",
        "num_heads = 4\n",
        "\n",
        "# Random input tensors\n",
        "query = torch.randn(batch_size, query_len, d_model)\n",
        "key = torch.randn(batch_size, key_len, d_model)\n",
        "value = torch.randn(batch_size, key_len, d_model)\n",
        "\n",
        "# Initialize and apply MultiHeadAttention\n",
        "attention = MultiHeadAttention(d_model, num_heads)\n",
        "output, attention_weights = attention(query, key, value)\n",
        "\n",
        "print(\"Output shape:\", output.shape)\n",
        "print(\"Attention weights shape:\", attention_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMhSue5J2kAE",
        "outputId": "6663ea60-f232-4f1b-d520-e54913d65b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 3, 16])\n",
            "Attention weights shape: torch.Size([2, 4, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyPp-21C2rTI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}